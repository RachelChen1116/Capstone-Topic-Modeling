{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict,Counter\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import sys, getopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class VAE(object):\n",
    "    \"\"\"\n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus,\n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        print('Learning Rate:', self.learning_rate)\n",
    "\n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]], name='input')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.h_dim = (network_architecture[\"n_z\"]) # had a float before\n",
    "        self.a = 1*np.ones((1 , self.h_dim)).astype(np.float32)                         # a    = 1\n",
    "        self.prior_mean = tf.constant((np.log(self.a).T-np.mean(np.log(self.a),1)).T)          # prior_mean  = 0\n",
    "        self.prior_var = tf.constant(  ( ( (1.0/self.a)*( 1 - (2.0/self.h_dim) ) ).T +       # prior_var = 0.99 + 0.005 = 0.995\n",
    "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a,1) ).T  )\n",
    "        self.prior_logvar = tf.log(self.prior_var)\n",
    "\n",
    "        self._create_network()\n",
    "        with tf.name_scope('cost'):\n",
    "            self._create_loss_optimizer()\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def _create_network(self):\n",
    "        \"\"\"\n",
    "        steps:\n",
    "        1. initialize weights\n",
    "        2. build recognition network\n",
    "        3. build reconstruction network\n",
    "        \"\"\"\n",
    "        n_z = self.network_architecture['n_z']\n",
    "        n_hidden_gener_1 = self.network_architecture['n_hidden_gener_1']\n",
    "        en1 = slim.layers.linear(self.x, self.network_architecture['n_hidden_recog_1'], scope='FC_en1')\n",
    "        en1 = tf.nn.softplus(en1, name='softplus1')\n",
    "        en2 = slim.layers.linear(en1,    self.network_architecture['n_hidden_recog_2'], scope='FC_en2')\n",
    "        en2 = tf.nn.softplus(en2, name='softplus2')\n",
    "        en2_do = slim.layers.dropout(en2, self.keep_prob, scope='en2_dropped')\n",
    "        self.posterior_mean   = slim.layers.linear(en2_do, self.network_architecture['n_z'], scope='FC_mean')\n",
    "        self.posterior_logvar = slim.layers.linear(en2_do, self.network_architecture['n_z'], scope='FC_logvar')\n",
    "        self.posterior_mean   = slim.layers.batch_norm(self.posterior_mean, scope='BN_mean')\n",
    "        self.posterior_logvar = slim.layers.batch_norm(self.posterior_logvar, scope='BN_logvar')\n",
    "\n",
    "        with tf.name_scope('z_scope'):\n",
    "            eps = tf.random_normal((self.batch_size, n_z), 0, 1,                            # take noise\n",
    "                                   dtype=tf.float32)\n",
    "            self.z = tf.add(self.posterior_mean,\n",
    "                            tf.multiply(tf.sqrt(tf.exp(self.posterior_logvar)), eps))         # reparameterization z\n",
    "            self.posterior_var = tf.exp(self.posterior_logvar) \n",
    "\n",
    "        p = slim.layers.softmax(self.z)\n",
    "        p_do = slim.layers.dropout(p, self.keep_prob, scope='p_dropped')               # dropout(softmax(z))\n",
    "        decoded = slim.layers.linear(p_do, n_hidden_gener_1, scope='FC_decoder')\n",
    "\n",
    "        self.x_reconstr_mean = tf.nn.softmax(slim.layers.batch_norm(decoded, scope='BN_decoder'))                    # softmax(bn(50->1995))\n",
    "\n",
    "        print(self.x_reconstr_mean)\n",
    "\n",
    "    def _create_loss_optimizer(self):\n",
    "\n",
    "        #self.x_reconstr_mean+=1e-10                                                     # prevent log(0)\n",
    "\n",
    "        NL = -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean+1e-10), 1)     # cross entropy on categorical\n",
    "        #reconstr_loss = -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean), 1)\n",
    "\n",
    "        var_division    = self.posterior_var  / self.prior_var\n",
    "        diff            = self.posterior_mean - self.prior_mean\n",
    "        diff_term       = diff * diff / self.prior_var\n",
    "        logvar_division = self.prior_logvar - self.posterior_logvar\n",
    "        KLD = 0.5 * (tf.reduce_sum(var_division + diff_term + logvar_division, 1) - self.h_dim )\n",
    "\n",
    "        self.cost = tf.reduce_mean(NL + KLD)\n",
    "\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.99).minimize(self.cost)\n",
    "\n",
    "    def partial_fit(self, X):\n",
    "\n",
    "        #if hasattr(self, 'decoder_weight'):\n",
    "            #decoder_weight = self.decoder_weight\n",
    "        #else:\n",
    "        decoder_weight = [v for v in tf.global_variables() if v.name=='FC_decoder/weights:0'][0]\n",
    "        opt, cost,emb = self.sess.run((self.optimizer, self.cost, decoder_weight),feed_dict={self.x: X,self.keep_prob: .8})\n",
    "        return cost,emb\n",
    "\n",
    "    def test(self, X):\n",
    "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
    "        \"\"\"\n",
    "        cost = self.sess.run((self.cost),feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
    "        return cost\n",
    "    def topic_prop(self, X):\n",
    "        \"\"\"heta_ is the topic proportion vector. Apply softmax transformation to it before use.\n",
    "        \"\"\"\n",
    "        theta_ = self.sess.run((self.z),feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
    "        return theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(data, min_length):\n",
    "    return np.bincount(data, minlength=min_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = pkl.load(open(\"Downloads/all_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pkl.load(open(\"Downloads/sent_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))  # nltk stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token\n",
    "    # token2id\n",
    "    token_counter = Counter(all_tokens)\n",
    "    token_counter = Counter({k:v for k,v in token_counter.items() if k not in stops and k.isalpha()})\n",
    "    # unzip the vocab and its corresponding count\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    # Give indices from 2 to the vocab\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    # Add pad and unk to vocab\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_tokens,max_vocab_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data, token2id, id2token):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = token2index_dataset(data, token2id, id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([np.array(document) for document in x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([onehot(doc.astype('int'),max_vocab_size+2) for doc in x_train if np.sum(doc)!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_tr = x_train.shape[0]\n",
    "docs_tr = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "learning_rate=0.002\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=100, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=100, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=x_train.shape[1], # 1st layer decoder neurons\n",
    "         n_input=x_train.shape[1], # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(layer1=100,layer2=100,num_topics=50,bs=200,eta=0.002):\n",
    "    tf.reset_default_graph()\n",
    "    network_architecture = \\\n",
    "        dict(n_hidden_recog_1=layer1, # 1st layer encoder neurons\n",
    "             n_hidden_recog_2=layer2, # 2nd layer encoder neurons\n",
    "             n_hidden_gener_1=x_train.shape[1], # 1st layer decoder neurons\n",
    "             n_input=x_train.shape[1], # MNIST data input (img shape: 28*28)\n",
    "             n_z=num_topics)  # dimensionality of latent space\n",
    "    batch_size=bs\n",
    "    learning_rate=eta\n",
    "    return network_architecture,batch_size,learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "\n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network_architecture, minibatches, type='prodlda',learning_rate=0.001,\n",
    "          batch_size=200, training_epochs=100, display_step=5):\n",
    "    tf.reset_default_graph()\n",
    "    vae = VAE(network_architecture,\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 batch_size=batch_size)\n",
    "    writer = tf.summary.FileWriter('logs', tf.get_default_graph())\n",
    "    emb=0\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = next(minibatches)\n",
    "            # Fit training using batch data\n",
    "            cost,emb = vae.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "\n",
    "            if np.isnan(avg_cost):\n",
    "                print(epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape)\n",
    "                print('Encountered NaN, stopping training. Please check the learning_rate settings and the momentum.')\n",
    "                # return vae,emb\n",
    "                sys.exit()\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    return vae,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print('---------------Printing the Topics------------------')\n",
    "    for i in range(len(beta)):\n",
    "        print(\" \".join([feature_names[j]\n",
    "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
    "    print('---------------End of Topics------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatches = create_minibatch(docs_tr.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_architecture,batch_size,learning_rate=make_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001\n",
      "Tensor(\"Softmax:0\", shape=(200, 10002), dtype=float32)\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0001 cost= 8267.231494141\n",
      "Epoch: 0006 cost= 5601.142812500\n",
      "Epoch: 0011 cost= 5395.203759766\n",
      "Epoch: 0016 cost= 5000.472275391\n",
      "Epoch: 0021 cost= 4888.346455078\n",
      "Epoch: 0026 cost= 4760.160297852\n",
      "Epoch: 0031 cost= 4598.640258789\n",
      "Epoch: 0036 cost= 4345.423139648\n",
      "Epoch: 0041 cost= 4217.210322266\n",
      "Epoch: 0046 cost= 4042.558408203\n",
      "Epoch: 0051 cost= 4036.144296875\n",
      "Epoch: 0056 cost= 3887.852172852\n",
      "Epoch: 0061 cost= 3861.446752930\n",
      "Epoch: 0066 cost= 3689.239399414\n",
      "Epoch: 0071 cost= 3586.152426758\n",
      "Epoch: 0076 cost= 3468.924501953\n",
      "Epoch: 0081 cost= 3454.514555664\n",
      "Epoch: 0086 cost= 3439.926684570\n",
      "Epoch: 0091 cost= 3265.541440430\n",
      "Epoch: 0096 cost= 3194.837392578\n"
     ]
    }
   ],
   "source": [
    "vae,emb = train(network_architecture, \n",
    "                minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Printing the Topics------------------\n",
      "cnn tuesday abortions tweeted scientists democrats president trump donald abortion\n",
      "comicbook tweeted tuesday cnn cbs donald wednesday subscribed lawmakers police\n",
      "appropriately thorough clarify translated destroying gem absent albeit ah slowed\n",
      "comicbook subscribed podcast commented airs theaters fans russo starred tweeted\n",
      "constitutional statute surveillance taxpayer treasury insurers reinsurance reform advocates requirement\n",
      "albeit distant disappear scholars absent <pad> gem inserted masses downward\n",
      "clauses contractual quizzes cardholder salutation throughs consents datonics gifs accountholder\n",
      "renters motorcycle proposal assets investor investment proposals coverage farmers recreational\n",
      "flowing absent appropriately divine someday descent <pad> instructed backwards ought\n",
      "occupation appropriately wonders <pad> gem abdominal dedication assumption inherently ought\n",
      "senate politicians democrats voters republicans constitutional legislation bloomberg proposal lawmakers\n",
      "magpul mag caliber chambered ammo mags cartridges ammoland magazines muzzle\n",
      "elections senator regime trump spokesman elected allegations supporters conservative warned\n",
      "dragon cersei arya dany westeros gown jaime winterfell sansa duchess\n",
      "tablespoon desserts mins homemade melted pour prep sprinkle chopped vanilla\n",
      "inserted appropriately observation scholars selfish flowing suburbs grasp wonders ah\n",
      "fcra authoritative relying validate flame crashing inserted scholars <pad> instructed\n",
      "comicbook subscribed podcast fans episode instagram wwe theaters headlines emotional\n",
      "proceeded couch buddy needless hung dumb rude dude stealing jerk\n",
      "screaming dude crap laugh suddenly jerk scared dad loud confused\n",
      "courier gary marlette pensacola sentinel andy knoxville fullscreen murphy charlie\n",
      "parenting insider advice firearm spend amendment teach shoppers thoughts mistake\n",
      "meanings clicked remarketing analyse incomplete obligated advertise instruct strive advertiser\n",
      "infotainment sedan headlights mirrors keyless seats trim leather steering grille\n",
      "verification documentation requested update request profile executives transactions representative verify\n",
      "<pad> competent relying researching fcra authoritative scholars appropriately absent presumably\n",
      "clauses contractual throughs datonics gifs compile originating browsers liveramp recipient\n",
      "inadequate engel eradicate hershey forest faso burkina ivory salwan labels\n",
      "desires establishing gem albeit wonders continuously skinny backgrounds scattered <pad>\n",
      "natalie imgur actresses photographed beatles celebrities actress diana bowie captured\n",
      "configuration efficiency connexity flow int operating load crossref experimental capabilities\n",
      "appropriately flowing <pad> worries gem regards uncertain prescribed rhythm personalities\n",
      "nat ml pathway analyses molecular biol rna metabolic synthesis mechanisms\n",
      "inserted skinny incoming verbal <pad> absent instruction masses simpler scholars\n",
      "<unk> dad dude guess telling walked somewhere trouble kid revenge\n",
      "warranties incidental copyrighted successors consequential devised sms expressly indemnify clasp\n",
      "<pad> albeit scholars appropriately pitched inserted tended deter smoothly blogs\n",
      "psychologists <pad> insists strangers assumption consciousness curse desires encountered depicted\n",
      "invoice imperfect refund delivery accepting incorrect denver forex nyse mattress\n",
      "draws <pad> inevitably regards absent motto flame flowing albeit scholars\n",
      "songs episodes movie fans television film album producer actors movies\n",
      "towing bodily uninsured motorist countrywide endorsement coverages applicants collision forgiveness\n",
      "defining inserted <pad> pitched scholars pitching motto torn counted rhythm\n",
      "verification documentation update requested request profile representative verify overview executives\n",
      "destroying rhythm therapists wonders <pad> absent behave flame promptly appropriately\n",
      "prepaid mbps mbb purch acct pymt excl svcs hotspot reqs\n",
      "parenting episode sing asleep weird fans scenes dear daughter fan\n",
      "comicbook subscribed theaters podcast franchise sequel endgame feige starring fans\n",
      "alcoholic noon chooses paramount waterfront polyurethane mattresses breathable coils directing\n",
      "psychologists wonders inserted <pad> specifics thorough appropriately lightning gem encountered\n",
      "---------------End of Topics------------------\n"
     ]
    }
   ],
   "source": [
    "print_top_words(emb, list(zip(*sorted(token2id.items(), key=lambda x: x[1])))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
